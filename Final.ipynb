{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxh7oeJWt6Xb"
   },
   "source": [
    "# Part 1 - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5DcgD--e4R0"
   },
   "source": [
    "Scripts to show python and NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "error",
     "timestamp": 1659807623622,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "tMudhyP6e2XW",
    "outputId": "30400969-43e1-4593-8f71-bcf4bc728a48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alisson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alisson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alisson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alisson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/alisson/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud as wc\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# path = \"/content/drive/MyDrive/_FACULDADE/7º Semestre/Tópicos VIII/_Final/V2/\"\n",
    "path = \"\"\n",
    "\n",
    "# sentence tokenize object\n",
    "sentences = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcIh3gdWiyjZ"
   },
   "source": [
    "Dealing with web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1659807342913,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "-lYmuzPGi2iS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en-core-web-md==3.4.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.0/en_core_web_md-3.4.0-py3-none-any.whl#egg=en_core_web_md==3.4.0 in /home/alisson/.local/lib/python3.8/site-packages (3.4.0)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/alisson/.local/lib/python3.8/site-packages (from en-core-web-md==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.21.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (45.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/alisson/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/alisson/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/alisson/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/alisson/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/alisson/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alisson/.local/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/alisson/.local/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (8.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_md\")  # en_core_web_sm en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 147,
     "status": "aborted",
     "timestamp": 1659807342921,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "ofzNQlK7jaCz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Ever noticed how plane seats appear to be gett...\n",
       "1      A drunk teenage boy had to be rescued by secur...\n",
       "2      Dougie Freedman is on the verge of agreeing a ...\n",
       "3      Liverpool target Neto is also wanted by PSG an...\n",
       "4      Bruce Jenner will break his silence in a two-h...\n",
       "                             ...                        \n",
       "995    Women travelling on the London Underground are...\n",
       "996    Millions of devout Christians around the world...\n",
       "997    Floyd Mayweather and Manny Pacquiao may only b...\n",
       "998    Nigel Farage suffered a fresh blow today after...\n",
       "999    More than 140 children have been removed from ...\n",
       "Name: article, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlData1 = f\"https://docs.google.com/spreadsheets/d/1sbp6Y1YZWHILHMma8qphe8tgJvDQ7s_ubLJhY2hSJI8/gviz/tq?tqx=out:csv&sheet=test\"\n",
    "urlData2 = f\"https://docs.google.com/spreadsheets/d/1CSIVmE1NKiui-EEEHRnnsXzXmy908mQRtasSmtQe7x8/gviz/tq?tqx=out:csv&sheet=validation\"\n",
    "\n",
    "df1 = pd.read_csv(urlData1)\n",
    "df2 = pd.read_csv(urlData2)\n",
    "\n",
    "news = df1.append(df2, ignore_index=True)['article'][:1000]\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 143,
     "status": "aborted",
     "timestamp": 1659807342924,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "QGHzihtqeLMK"
   },
   "outputs": [],
   "source": [
    "treatment = False\n",
    "wordcloud = False\n",
    "gridSearch = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSPyyFVvs6tu"
   },
   "source": [
    "Removing Blacklist words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "error",
     "timestamp": 1659807342971,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "clIYjYsCs-9j",
    "outputId": "b7d45812-655e-428b-cd98-309ee342a88a"
   },
   "outputs": [],
   "source": [
    "if treatment:\n",
    "  blackList = [\"cnn\", \"bbc\", \"(CNN)\", '\\xad', \"abc news\", \"\\u200b\", \"\\u202c\",\n",
    "                \"\\u202a#\", \"\\u200e\", \"\\u202c\", \"whyimnotvotingforhillary\", \"\\\"\\'\", \"00pm\", \"042secs\", \"10\", '000', '11', '111th', '112million', '117million', '12th', '130', '130million', '13th', '140billion', '140million', '147notjustanumber', '14billion', '14th', '15', '15am', '15th', '16th', '17', '177billion', '17th', '18', '1800', '180billion', '1850', '1870', '18s', '18th', '19', '1920', '1930', '1940', '1960', '1970', '1980', '1990', '19s', '19th', '20', '2000', '208', '20st', '20th', '21inch', '23rd', '24', '25st', '27', '27pm', '280sl', '2w', '30', '300_missile', '300er', '30am', '30pm', '30th', '31pm', '31st', '33minute', '35million', '363ad', '368million', '377million', '38', '385million', '390million', '39th', '3_contest', '3d', '3lbs', '405nm', '42million', '45min', '47', '49th', '4million', '506million', '50metre', '50pm', '50th', '52pm', '545million', '55s', '59', '5lbs', '5million', '617s', '61ad', '64th', '65th', '669million', '6l', '6lbs', '6music', '765million', '7million', '82pt', '84pt', '85pt', '8lbs', '8mmm', '8mmm_aboriginal', '90', '979pp', '9billion', '00', '09', '0kms', '3_contest', '45pm', '60s', '61', '7sqm']\n",
    "  blackList = list(map(lambda a: \"(?i)\" + a, blackList))\n",
    "  news.replace(blackList, \"\", inplace=True, regex=True)\n",
    "\n",
    "  for i in range(len(news)):\n",
    "      news[i] = re.sub(\n",
    "          r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\", \"\", news[i])\n",
    "\n",
    "  news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1659807343130,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "chk8wbTwG0vV"
   },
   "outputs": [],
   "source": [
    "if treatment:\n",
    "  dlemma=[]\n",
    "  for d in news:\n",
    "    tdoc=nlp(d)\n",
    "    lm=[\" \".join([token.lemma_.lower() for token in tdoc if \n",
    "      token.pos_ in ['NOUN','PROPN'] and \n",
    "      token.pos_ not in ['TIME', '-PRON-', 'CARDINAL', 'ORDINAL'] and\n",
    "      token.is_stop == False and \n",
    "      token.is_punct == False and \n",
    "      token.is_digit == False and \n",
    "      len(token.lemma_) > 3])] \n",
    "    dlemma.append(lm)\n",
    "  dlemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1659807343148,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "RVXuetnhl2fk"
   },
   "outputs": [],
   "source": [
    "if treatment:\n",
    "  tokens = []\n",
    "  for doc in dlemma:\n",
    "      tokens.append(doc[0].split())\n",
    "  tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 343,
     "status": "aborted",
     "timestamp": 1659807343159,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "acG00DfP8743"
   },
   "outputs": [],
   "source": [
    "if treatment:\n",
    "  phrases = Phrases(tokens, min_count=2)\n",
    "  bigram = Phraser(phrases)\n",
    "  bdocs = [bigram[d] for d in tokens]\n",
    "\n",
    "  # [print(d) for d in bdocs]\n",
    "  # for d in bdocs:\n",
    "  #   for w in d:\n",
    "  #     if '_' in w:\n",
    "  #       print(w)\n",
    "\n",
    "  tokens = bdocs\n",
    "\n",
    "  with open(path+'tokens.txt', 'w') as temp_file:\n",
    "    for row in tokens:\n",
    "      for index, item in enumerate(row):\n",
    "        if index == len(row)-1:\n",
    "          temp_file.write(\"%s\\n\" % item)\n",
    "        else:\n",
    "          temp_file.write(\"%s,\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPbIlYHpqN8R"
   },
   "source": [
    "### Statistcs\n",
    "\n",
    "- N of documents\n",
    "- the shortest one (n of words)\n",
    "- the longest one (n of words)\n",
    "- the average size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "aborted",
     "timestamp": 1659807343164,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "77XdDdPzrLHg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents: 1000\n",
      "shortest: 31\n",
      "largest: 514\n",
      "average: 171.645\n"
     ]
    }
   ],
   "source": [
    "file = open(path+'tokens.txt', 'r')\n",
    "lines = file.readlines()\n",
    "tokens = [i.replace(\"\\n\", \"\").split(\",\") for i in lines]\n",
    "\n",
    "n_documents = len(tokens)\n",
    "shortest = min([len(d) for d in tokens])\n",
    "largest = max([len(d) for d in tokens])\n",
    "average = sum([len(d) for d in tokens]) / n_documents\n",
    "\n",
    "print(\"number of documents: {}\\nshortest: {}\\nlargest: {}\\naverage: {}\".format(n_documents, shortest, largest, average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "aborted",
     "timestamp": 1659807343167,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "T0__l9DCpS4k"
   },
   "outputs": [],
   "source": [
    "if wordcloud:\n",
    "  coll = ''\n",
    "  allTokens = []\n",
    "  dictionaryOfString = []\n",
    "\n",
    "  for d in tokens:\n",
    "    [allTokens.append(w) for w in d]\n",
    "    collect = ' '.join([w for w in d])\n",
    "    dictionaryOfString.append(collect)\n",
    "    coll = coll+' '+collect\n",
    "\n",
    "  mycloud = wc.WordCloud().generate(coll)\n",
    "  plt.figure(figsize=(20, 30))\n",
    "  plt.imshow(mycloud)\n",
    "\n",
    "  setOfTokens = set(allTokens)\n",
    "\n",
    "  print(\"n of words of dictionary: {}\\nn of words of collection: {}\".format(\n",
    "    len(setOfTokens), len(allTokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3b6shEbuAaZ"
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the saved tokens as `corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "aborted",
     "timestamp": 1659807343170,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "6ryk9Z0iKnoL"
   },
   "outputs": [],
   "source": [
    "corpus = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZspPdYDKnoL"
   },
   "source": [
    "## Tomotopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tomotopy to get the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 329,
     "status": "aborted",
     "timestamp": 1659807343174,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "l-D8mJAgKnoL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tomotopy in /home/alisson/.local/lib/python3.8/site-packages (0.12.3)\r\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/alisson/.local/lib/python3.8/site-packages (from tomotopy) (1.21.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tomotopy\n",
    "import tomotopy as tp\n",
    "import numpy as np\n",
    "ldamodel = tp.LDAModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a type of gridSearch to get the best params (as the professor's example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 324,
     "status": "aborted",
     "timestamp": 1659807343177,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "CX0uxR7kKnoL"
   },
   "outputs": [],
   "source": [
    "rm_top =[30] #the number of top words to be removed (default 0)\n",
    "min_df = [0] #minimum document frequency of words (default 0)\n",
    "min_cf = [0] #minimum collection frequency of words. (default 0)\n",
    "alphas = [0.45] #hyperparameter of Dirichlet distribution for document-topic (default 0.1)\n",
    "etas = [0.45]  #hyperparameter of Dirichlet distribution for topic-word (default 0.01)\n",
    "K=[43]\n",
    "if gridSearch:\n",
    "    cv=[]\n",
    "    print('# of docs:', len(corpus)) \n",
    "    for k in K:\n",
    "      for mdf in min_df:\n",
    "        for rm in rm_top:\n",
    "          for a in alphas: \n",
    "            for e in etas:\n",
    "              for mcf in min_cf:\n",
    "                #create an object\n",
    "                #tw term weight IDF (Inverse Document Frequency term weighting), ONE (equal - default), PMI (Use Pointwise Mutual Information term weighting)\n",
    "                LDA = ldamodel(tw = tp.TermWeight.IDF, k=k, alpha = a , eta = e, seed = 1,  min_df = mdf, min_cf = mcf, rm_top = rm)\n",
    "                #add documents to it\n",
    "                for doc in corpus:\n",
    "                    LDA.add_doc(doc)\n",
    "                #train\n",
    "                LDA.train(iter = 500) # iter (# of iterations - default 10) # workers (# of cores to be used - 0 all)\n",
    "                #get the coherence (c_v)\n",
    "                coh = tp.coherence.Coherence(LDA, coherence='c_v')\n",
    "                average_coherence = coh.get_score()\n",
    "                print('K: %2d mcf: %2d mdf: %2d rm: %2d alfa: %.3f beta: %.3f coherence: %.3f'%(k,mcf,mdf,rm,a,e,average_coherence),end=' Collection ')\n",
    "                print(' Vocab size:', len(LDA.used_vocabs), ' # of words:', LDA.num_words)\n",
    "                cv.append(average_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model and printing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "aborted",
     "timestamp": 1659807343182,
     "user": {
      "displayName": "Alisson Luan de Lima Peloso",
      "userId": "13242966935980600049"
     },
     "user_tz": 180
    },
    "id": "x2HqULJwOOIr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 => masters nepal kathmandu hole woods augusta tiger_woods birdie golf quake \n",
      "Topic 1 => pylon lion gallipoli animal elephant jackson measle pool safechuck tomb \n",
      "Topic 2 => death student court friend officer girl report victim daughter wife \n",
      "Topic 3 => hatch rangers lynch kukucova tariq fifa trent ceglia heatlie gemini \n",
      "Topic 4 => hamilton race rosberg gazprom mercedes dunford racing grand_prix prix mccoy \n",
      "Topic 5 => pusok gray brain manyang santana drug depression ssri flakka device \n",
      "Topic 6 => toure napoli pardew palermo juventus lazio watford serie dybala ince \n",
      "Topic 7 => luke goff property bates water water_right secret_service albin hall garage \n",
      "Topic 8 => weinstein protein_world starbucks poster harry_potter qaeda recipe arousal bines chemical \n",
      "Topic 9 => indiana freedom teacher pupil hutchinson extremism arkansas chimpanzee huntsman value \n",
      "Topic 10 => bilby fairy_circle costello spacecraft sandstorm mason affleck mission asteroid mercury \n",
      "Topic 11 => mall lazarus wallenda miss_buckley cave buckley pereiro_mendez rhodri vigil nightclub \n",
      "Topic 12 => janner dooley marijuana sadek tidal tinder informant song music user \n",
      "Topic 13 => bournemouth ukraine russia evans sarkozy hardy doherty putin klitschko ipswich \n",
      "Topic 14 => lilly_pulitzer hilbert camel stringer canal ecuador argentina swain lazar sunset \n",
      "Topic 15 => dinosaur gianna brandt alpha_delta surrogate animal chilesaurus botox ashley_madison bird \n",
      "Topic 16 => hair mathieson festival mcintyre_comment rocket anzac clitheroe spacex medium_user tweet \n",
      "Topic 17 => patient doctor tumour kidney cancer treatment surgery shapps harley hospital \n",
      "Topic 18 => plane airport flight aircraft passenger airline pilot glenna ryanair aoki \n",
      "Topic 19 => steele dellinger diana casa_campo prasanna doody cobain melgen jenner surrogacy \n",
      "Topic 20 => abortion clark benaud oduwole nigeria heath jemima captain_rahmani guffick kenya \n",
      "Topic 21 => johannesburg jesus bernard south_africa white_house national ossuary rommel violence immigrant \n",
      "Topic 22 => hernandez pacquiao dellal herpe mayweather easter_roll lloyd fight chan floyd_mayweather \n",
      "Topic 23 => chapple yarmouk huminski jeep ledge calais chloe wood curry puppy \n",
      "Topic 24 => spell salcombe alcorn christie mooney thief edwards arnold wiggins hsbc \n",
      "Topic 25 => blasio church rocket pg&e uzzell penitent fire nathan holy_week pothole \n",
      "Topic 26 => knight sloan sprouse williams lapointe gordon brokaw curry filipovic eden \n",
      "Topic 27 => robot laser nathalie gavern character pedestrian mateo chihira baby tokyo \n",
      "Topic 28 => ouimette tong comcast sambora clooney julia cousins merger cleveland fish \n",
      "Topic 29 => dress clinton perry fashion designer design wilson shoe nadal benghazi \n",
      "Topic 30 => execution drug death_penalty stancil station gravity gloucester skull carwyn spacecraft \n",
      "Topic 31 => juarez joelison mchenry mccombs michelle frankie musick shopper darpa lunch \n",
      "Topic 32 => mann grand froch isis briel kassab sausage_roll islamic_state rose bush \n",
      "Topic 33 => mountain poldark avalanche snow climber base_camp nail_house everest celtic xiong \n",
      "Topic 34 => labour party miliband election david_cameron ukip clegg hour_contract sturgeon tory \n",
      "Topic 35 => jenna mcindoe west_indies graham polkinghorne rutherford wicket google hinch anderson \n",
      "Topic 36 => wigan mackay beatrice millwall scooby afobe backpacker mirallas mccarthy milat \n",
      "Topic 37 => west_brom crystal_palace dysart doll sutcliffe bolasie pulis ferrari broadmoor stiviano \n",
      "Topic 38 => weather schieffer storm temperature manly protester sydney rain reclaim_australia degree_shower \n",
      "Topic 39 => mote dress munroe miss_bonas turing andreja model hair lindsay mantellato \n",
      "Topic 40 => iran yemen boat volcano migrant ship deal agreement vessel aden \n",
      "Topic 41 => arsenal chelsea liverpool premier_league united england match ball football sterling \n",
      "Topic 42 => kelly_anne breast_cancer toutouni north_charleston slager body_camera zhai murray cancer follower \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63253/3175099011.py:6: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  LDA.train(iter = 500) # iter (# of iterations - default 10) # workers (# of cores to be used - 0 all)\n"
     ]
    }
   ],
   "source": [
    "LDA = ldamodel(tw = tp.TermWeight.IDF, k=K[0], alpha = alphas[0] , eta = etas[0], seed = 1,  min_df = min_df[0], min_cf = min_cf[0], rm_top = rm_top[0])\n",
    "#add documents to it\n",
    "for doc in corpus:\n",
    "    LDA.add_doc(doc)\n",
    "#train\n",
    "LDA.train(iter = 500) # iter (# of iterations - default 10) # workers (# of cores to be used - 0 all)\n",
    "\n",
    "for i in range(K[0]):\n",
    "    print(\"Topic\", i, end=' => ')\n",
    "    for w in LDA.get_topic_words(i):\n",
    "        print(w[0], end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There some topics that can be the same, for example topics of sports or crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
